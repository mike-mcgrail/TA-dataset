{"alert_action_builder": {"modular_alerts": [{"description": "Send to DataSet", "largeIcon": "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", "smallIcon": "iVBORw0KGgoAAAANSUhEUgAAACQAAAAkCAYAAADhAJiYAAAAAXNSR0IArs4c6QAAAw9JREFUWEe9mEuPTUEQx391E+wMFl4LC1+BBcbCYxIJ5iN4syFWLNh4fARhLx6bYV58ABI+AMIYK69ZiBViPCJoqT7dd86c2/ec7j43endz+tT99b+q61SVYJfZA+Ys0Cl+96zPIBdBXoQfmyXAeTDDfd7X/5iBjtr42n8PCBiBv1dBTtVtBOZA9oE8791nloN5AmyssWGAaZADIN/67fNA10BONgDp43cgo71QUUDWFQ7qUD+lUoHU6HuQvSAzCweIBvJQUyAHQ0rlAAWUSgIqQ6lS82XP5AKpjbcg+wulkoE81ATI4bJSbYA81GjhxsagDoWoxtSkg7JKtQVSG28Kg+YWsCHiYlS3KNQ4yBFVSoqnfy6DXMgw5l/5BSxt8b4BcwM6xxyQWQFmHNidafQ3mJtFnmJNpo1XIJsckI2xIQc1kmHwZ5EK+A7mXiZUFchCrXRQuxKhHJA8BLMNzGQGVAioq9REovtKQNbGFqfU6oSD9QOyBleBuQvEKlUBsjZSlaoD6iql0sdABYC6UFNAjFJNQF2l9PbtbJC+D5C1MQxGQ6Dp9sUAWYMxKeELyA6Qp2Fwsx3MHWBdzcEeg4yUrn2dBmYZsL7I7MH1A+RDvYp6MIYCezRT6/qkJUkskGbhtSUgNVJ+V1328T8BRblMS1x12bMWLnsU4TJ7/WODWsvbB71AAwtq+ynRK9t0w5RBXRYAsrloENc+KzFWgAaWGK0ysQnRe6iikP10TEfknrKHB/5xdQqZrc5NTYmwGm4DLz+0FpoHcz9RGQ9WBoq62nVpZhAF2izIZl/CXipa5eylJay205GJtud/tIS9Dp0Tvsi/AnI6E+e1K/Jv5xf5Zgw6x12Rb3v7XCDtOLQNmmvRBnU7DhWkjUIKozdrNr9R1CKwczTUKKYq5JTx/X1y56ofZ+1atRcLttIpQDoB0WHDy4WYSwJa1BhW4zbVZdrP6zimMriKBuppndsABcYw3lwUkIfRiUfdwMqWqSNgztSM9LRE1Xa7NBMqn82O9M6BlqrBpXnGj/QWxUx19z+mEFK9KzVMHQAAAABJRU5ErkJggg==", "label": "DataSet Event", "short_name": "dataset_event", "parameters": [{"format_type": "text", "required": true, "name": "dataset_serverhost", "label": "ServerHost", "default_value": "splunk", "help_string": "DataSet serverHost", "type": "", "value": "splunk"}, {"format_type": "text", "required": true, "name": "dataset_message", "label": "DataSet Message Body", "default_value": "$name$", "help_string": "Message sent to DataSet", "type": "", "value": "this is a test"}, {"format_type": "dropdownlist", "required": true, "name": "dataset_severity", "label": "Severity", "default_value": "3", "help_string": "Define DataSet Severity", "possible_values": {"1": "1", "2": "2", "3": "3", "4": "4", "5": "5", "6": "6"}, "type": "", "value": "3"}], "code": "import requests\nimport json\nimport datetime\nimport uuid\n\n# encoding = utf-8\n#set DataSet API url based on environment\ndef set_url(helper):\n    if helper.get_global_setting('dataset_eu_environment') == True:\n        return 'https://app.eu.scalyr.com/api/addEvents'\n    else:\n        return 'https://app.scalyr.com/api/addEvents'\n\n\ndef process_event(helper, *args, **kwargs):\n    \"\"\"\n    # IMPORTANT\n    # Do not remove the anchor macro:start and macro:end lines.\n    # These lines are used to generate sample code. If they are\n    # removed, the sample code will not be updated when configurations\n    # are updated.\n\n    [sample_code_macro:start]\n\n    # The following example gets and sets the log level\n    helper.set_log_level(helper.log_level)\n\n    # The following example gets the setup parameters and prints them to the log\n    dataset_eu_environment = helper.get_global_setting(\"dataset_eu_environment\")\n    helper.log_info(\"dataset_eu_environment={}\".format(dataset_eu_environment))\n    dataset_log_read_access_key = helper.get_global_setting(\"dataset_log_read_access_key\")\n    helper.log_info(\"dataset_log_read_access_key={}\".format(dataset_log_read_access_key))\n    dataset_log_write_access_key = helper.get_global_setting(\"dataset_log_write_access_key\")\n    helper.log_info(\"dataset_log_write_access_key={}\".format(dataset_log_write_access_key))\n\n    # The following example gets the alert action parameters and prints them to the log\n    dataset_serverhost = helper.get_param(\"dataset_serverhost\")\n    helper.log_info(\"dataset_serverhost={}\".format(dataset_serverhost))\n\n    dataset_message = helper.get_param(\"dataset_message\")\n    helper.log_info(\"dataset_message={}\".format(dataset_message))\n\n    dataset_severity = helper.get_param(\"dataset_severity\")\n    helper.log_info(\"dataset_severity={}\".format(dataset_severity))\n\n\n    # The following example adds two sample events (\"hello\", \"world\")\n    # and writes them to Splunk\n    # NOTE: Call helper.writeevents() only once after all events\n    # have been added\n    helper.addevent(\"hello\", sourcetype=\"sample_sourcetype\")\n    helper.addevent(\"world\", sourcetype=\"sample_sourcetype\")\n    helper.writeevents(index=\"summary\", host=\"localhost\", source=\"localhost\")\n\n    # The following example gets the events that trigger the alert\n    events = helper.get_events()\n    for event in events:\n        helper.log_info(\"event={}\".format(event))\n\n    # helper.settings is a dict that includes environment configuration\n    # Example usage: helper.settings[\"server_uri\"]\n    helper.log_info(\"server_uri={}\".format(helper.settings[\"server_uri\"]))\n    [sample_code_macro:end]\n    \"\"\"\n    \n    helper.set_log_level(helper.log_level)\n    helper.log_debug(\"Alert action dataset_event started.\")\n\n    # TODO: Implement your alert action logic here\n    ds_uuid = str(uuid.uuid4())\n    ds_url = set_url(helper)\n    ds_api_key = helper.get_global_setting('dataset_log_write_access_key')\n    ds_headers = { \"Authorization\": \"Bearer \" + ds_api_key }\n    dataset_serverhost = helper.get_param('dataset_serverhost')\n    dataset_severity = int(helper.get_param('dataset_severity'))\n    dataset_message = helper.get_param('dataset_message')\n    \n    events = helper.get_events()\n    counter = 1\n    \n    for event in events:\n\n        #convert ISO 8601 to epoch nanoseconds\n        epoch_time = datetime.datetime.strptime(event['_time'],\n                             \"%Y-%m-%dT%H:%M:%S.%f%z\")\n        ds_time = epoch_time.timestamp() * 1000000000\n        ds_timestamp = str(\"%.0f\" % ds_time)\n        \n        #format payload for DataSet addEvents API\n        ds_event_dict = {\n            \"session\": ds_uuid,\n            \"sessionInfo\": {\n                \"serverHost\": dataset_serverhost\n            },\n            \"events\": [\n                {\n                    \"thread\": str(counter),\n                    \"ts\": ds_timestamp,\n                    \"sev\": dataset_severity,\n                    \"tag\": \"splunk\",\n                    \"attrs\": {\n                        \"message\": dataset_message,\n                        \"Application\": \"splunk\"\n                    }\n                }\n            ],\n            \"threads\": [\n                {\n                    \"id\": counter,\n                    \"name\": \"splunk alert \" + str(counter)\n                }\n            ]\n        }\n        \n        ds_payload = json.loads(json.dumps(ds_event_dict))\n        counter +=1\n        \n        try:\n            r = requests.post(url=ds_url, headers=ds_headers, json=ds_payload)\n            \n            helper.log_debug(\"response = %s\" % r.text)\n            \n        except Exception as e:\n            helper.log_error(e)", "uuid": "45937b4f2a054c948b1a8357b133178d"}]}, "basic_builder": {"appname": "TA-dataset", "friendly_name": "DataSet Add-on for Splunk", "version": "1.0.0", "author": "MikeMcGrail", "description": "Add-on to integrate with DataSet by SentinelOne", "theme": "#0000F4", "large_icon": "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", "small_icon": "iVBORw0KGgoAAAANSUhEUgAAACQAAAAkCAYAAADhAJiYAAAAAXNSR0IArs4c6QAAAw9JREFUWEe9mEuPTUEQx391E+wMFl4LC1+BBcbCYxIJ5iN4syFWLNh4fARhLx6bYV58ABI+AMIYK69ZiBViPCJoqT7dd86c2/ec7j43endz+tT99b+q61SVYJfZA+Ys0Cl+96zPIBdBXoQfmyXAeTDDfd7X/5iBjtr42n8PCBiBv1dBTtVtBOZA9oE8791nloN5AmyssWGAaZADIN/67fNA10BONgDp43cgo71QUUDWFQ7qUD+lUoHU6HuQvSAzCweIBvJQUyAHQ0rlAAWUSgIqQ6lS82XP5AKpjbcg+wulkoE81ATI4bJSbYA81GjhxsagDoWoxtSkg7JKtQVSG28Kg+YWsCHiYlS3KNQ4yBFVSoqnfy6DXMgw5l/5BSxt8b4BcwM6xxyQWQFmHNidafQ3mJtFnmJNpo1XIJsckI2xIQc1kmHwZ5EK+A7mXiZUFchCrXRQuxKhHJA8BLMNzGQGVAioq9REovtKQNbGFqfU6oSD9QOyBleBuQvEKlUBsjZSlaoD6iql0sdABYC6UFNAjFJNQF2l9PbtbJC+D5C1MQxGQ6Dp9sUAWYMxKeELyA6Qp2Fwsx3MHWBdzcEeg4yUrn2dBmYZsL7I7MH1A+RDvYp6MIYCezRT6/qkJUkskGbhtSUgNVJ+V1328T8BRblMS1x12bMWLnsU4TJ7/WODWsvbB71AAwtq+ynRK9t0w5RBXRYAsrloENc+KzFWgAaWGK0ysQnRe6iikP10TEfknrKHB/5xdQqZrc5NTYmwGm4DLz+0FpoHcz9RGQ9WBoq62nVpZhAF2izIZl/CXipa5eylJay205GJtud/tIS9Dp0Tvsi/AnI6E+e1K/Jv5xf5Zgw6x12Rb3v7XCDtOLQNmmvRBnU7DhWkjUIKozdrNr9R1CKwczTUKKYq5JTx/X1y56ofZ+1atRcLttIpQDoB0WHDy4WYSwJa1BhW4zbVZdrP6zimMriKBuppndsABcYw3lwUkIfRiUfdwMqWqSNgztSM9LRE1Xa7NBMqn82O9M6BlqrBpXnGj/QWxUx19z+mEFK9KzVMHQAAAABJRU5ErkJggg==", "visible": true, "tab_version": "4.1.1", "tab_build_no": "0", "build_no": 1}, "data_input_builder": {"datainputs": [{"index": "default", "sourcetype": "dataset:query", "interval": "60", "use_external_validation": true, "streaming_mode_xml": true, "name": "dataset_query", "title": "DataSet Query", "description": "Retrieve events from DataSet API", "type": "customized", "parameters": [{"name": "start_time", "label": "Start Time", "help_string": "Start time for the DataSet query to use. Use shortform (e.g.: 1m, 24h, 3d).", "required": true, "format_type": "text", "default_value": "1m", "placeholder": "1m", "type": "text", "value": "1m"}, {"name": "end_time", "label": "End Time", "help_string": "If left blank, present time at query execution is used.", "required": false, "format_type": "text", "default_value": "", "placeholder": "", "type": "text", "value": ""}, {"name": "dataset_query_string", "label": "DataSet Query String", "help_string": "If left blank, all records (limited by max count) are retrieved.", "required": false, "format_type": "text", "default_value": "", "placeholder": "", "type": "text", "value": ""}, {"name": "max_count", "label": "Max Count", "help_string": "Specifies the maximum number of records to return, from 1 to 5000. If left blank, the default is 100.", "required": false, "format_type": "text", "default_value": "", "placeholder": "", "type": "text", "value": ""}], "data_inputs_options": [{"type": "customized_var", "name": "start_time", "title": "Start Time", "description": "Start time for the DataSet query to use. Use shortform (e.g.: 1m, 24h, 3d).", "required_on_edit": false, "required_on_create": true, "format_type": "text", "default_value": "1m", "placeholder": "1m"}, {"type": "customized_var", "name": "end_time", "title": "End Time", "description": "If left blank, present time at query execution is used.", "required_on_edit": false, "required_on_create": false, "format_type": "text", "default_value": "", "placeholder": ""}, {"type": "customized_var", "name": "dataset_query_string", "title": "DataSet Query String", "description": "If left blank, all records (limited by max count) are retrieved.", "required_on_edit": false, "required_on_create": false, "format_type": "text", "default_value": "", "placeholder": ""}, {"type": "customized_var", "name": "max_count", "title": "Max Count", "description": "Specifies the maximum number of records to return, from 1 to 5000. If left blank, the default is 100.", "required_on_edit": false, "required_on_create": false, "format_type": "text", "default_value": "", "placeholder": ""}], "code": "# encoding = utf-8\n\nimport os\nimport sys\nimport time\nimport datetime\nimport json\nimport requests\nfrom collections import OrderedDict\n'''\n    IMPORTANT\n    Edit only the validate_input and collect_events functions.\n    Do not edit any other part in this file.\n    This file is generated only once when creating the modular input.\n'''\n'''\n# For advanced users, if you want to create single instance mod input, uncomment this method.\ndef use_single_instance_mode():\n    return True\n'''\n\ndef validate_input(helper, definition):\n    \"\"\"Implement your own validation logic to validate the input stanza configurations\"\"\"\n    # This example accesses the modular input variable\n    # start_time = definition.parameters.get('start_time', None)\n    # end_time = definition.parameters.get('end_time', None)\n    # dataset_querystring = definition.parameters.get('dataset_query_string', None)\n    # max_count = definition.parameters.get('max_count', None)\n    pass\n\n\n#set DataSet API url based on environment\ndef set_url(helper):\n    if helper.get_global_setting('dataset_eu_environment') == True:\n        return 'https://app.eu.scalyr.com/api/query'\n    else:\n        return 'https://app.scalyr.com/api/query'\n\n\n#get checkpoint, set initial value to 0 if nonexistent\ndef get_checkpoint(helper, key):\n    state = helper.get_check_point(key)\n\n    #if state exists, return it\n    if (state not in [None,'']):\n        return state\n    #if nonexistent, set to 0 for 1st iteration\n    else:\n        helper.log_info(\"checkpoint is 0\")\n        return 0\n\n\n#compare timestamp to checkpoint value\ndef compare_checkpoint(helper, key, data):\n    checkpoint_time = get_checkpoint(helper, key)\n    event_time = data\n\n    if checkpoint_time == 0 or (event_time > checkpoint_time):\n        return True\n    else:\n        helper.log_info(\"skipping due to event_time=%s is less than checkpoint=%s\" % (str(event_time), str(checkpoint_time)))\n        return False\n\n\n#parse proxy configuraton for requests module\ndef set_proxy(helper, ew):\n    if helper.get_proxy():\n        helper.log_debug(\"proxy enabled=true\")\n        proxy = helper.get_proxy()\n        proxies = {}\n        if proxy['proxy_username'] and proxy['proxy_password']:\n            proxies['http'] = proxy['proxy_username'] + \":\" + proxy['proxy_password'] + \"@\" + proxy['proxy_url'] + \":\" + proxy['proxy_port']\n        elif proxy['proxy_username']:\n            proxies['http'] = proxy['proxy_username'] + \"@\" + proxy['proxy_url'] + \":\" + proxy['proxy_port']\n        else:\n            proxies['http'] = proxy['proxy_url'] + proxy['proxy_port']\n        if proxy['proxy_type'] != 'http':\n            proxies['http'] = proxy['proxy_type'] + \"://\" + proxies['http']\n        \n        proxies['https'] = proxies['http']\n        return proxies\n        \n    else:\n        helper.log_debug(\"proxy enabled=false\")\n        return None\n        \n\ndef collect_events(helper, ew):\n    \"\"\"Implement your data collection logic here\n\n    # The following examples get the arguments of this input.\n    # Note, for single instance mod input, args will be returned as a dict.\n    # For multi instance mod input, args will be returned as a single value.\n    opt_start_time = helper.get_arg('start_time')\n    opt_end_time = helper.get_arg('end_time')\n    opt_dataset_query_string = helper.get_arg('dataset_query_string')\n    opt_max_count = helper.get_arg('max_count')\n    # In single instance mode, to get arguments of a particular input, use\n    opt_start_time = helper.get_arg('start_time', stanza_name)\n    opt_end_time = helper.get_arg('end_time', stanza_name)\n    opt_dataset_query_string = helper.get_arg('dataset_query_string', stanza_name)\n    opt_max_count = helper.get_arg('max_count', stanza_name)\n\n    # get input type\n    helper.get_input_type()\n\n    # The following examples get input stanzas.\n    # get all detailed input stanzas\n    helper.get_input_stanza()\n    # get specific input stanza with stanza name\n    helper.get_input_stanza(stanza_name)\n    # get all stanza names\n    helper.get_input_stanza_names()\n\n    # The following examples get options from setup page configuration.\n    # get the loglevel from the setup page\n    loglevel = helper.get_log_level()\n    # get proxy setting configuration\n    proxy_settings = helper.get_proxy()\n    # get account credentials as dictionary\n    account = helper.get_user_credential_by_username(\"username\")\n    account = helper.get_user_credential_by_id(\"account id\")\n    # get global variable configuration\n    global_dataset_eu_environment = helper.get_global_setting(\"dataset_eu_environment\")\n    global_dataset_log_read_access_key = helper.get_global_setting(\"dataset_log_read_access_key\")\n    global_dataset_log_write_access_key = helper.get_global_setting(\"dataset_log_write_access_key\")\n\n    # The following examples show usage of logging related helper functions.\n    # write to the log for this modular input using configured global log level or INFO as default\n    helper.log(\"log message\")\n    # write to the log using specified log level\n    helper.log_debug(\"log message\")\n    helper.log_info(\"log message\")\n    helper.log_warning(\"log message\")\n    helper.log_error(\"log message\")\n    helper.log_critical(\"log message\")\n    # set the log level for this modular input\n    # (log_level can be \"debug\", \"info\", \"warning\", \"error\" or \"critical\", case insensitive)\n    helper.set_log_level(log_level)\n\n    # The following examples send rest requests to some endpoint.\n    response = helper.send_http_request(url, method, parameters=None, payload=None,\n                                        headers=None, cookies=None, verify=True, cert=None,\n                                        timeout=None, use_proxy=True)\n    # get the response headers\n    r_headers = response.headers\n    # get the response body as text\n    r_text = response.text\n    # get response body as json. If the body text is not a json string, raise a ValueError\n    r_json = response.json()\n    # get response cookies\n    r_cookies = response.cookies\n    # get redirect history\n    historical_responses = response.history\n    # get response status code\n    r_status = response.status_code\n    # check the response status, if the status is not sucessful, raise requests.HTTPError\n    response.raise_for_status()\n\n    # The following examples show usage of check pointing related helper functions.\n    # save checkpoint\n    helper.save_check_point(key, state)\n    # delete checkpoint\n    helper.delete_check_point(key)\n    # get checkpoint\n    state = helper.get_check_point(key)\n\n    # To create a splunk event\n    helper.new_event(data, time=None, host=None, index=None, source=None, sourcetype=None, done=True, unbroken=True)\n    \"\"\"\n\n    '''\n    # The following example writes a random number as an event. (Multi Instance Mode)\n    # Use this code template by default.\n    import random\n    data = str(random.randint(0,100))\n    event = helper.new_event(source=helper.get_input_type(), index=helper.get_output_index(), sourcetype=helper.get_sourcetype(), data=data)\n    ew.write_event(event)\n    '''\n\n    '''\n    # The following example writes a random number as an event for each input config. (Single Instance Mode)\n    # For advanced users, if you want to create single instance mod input, please use this code template.\n    # Also, you need to uncomment use_single_instance_mode() above.\n    import random\n    input_type = helper.get_input_type()\n    for stanza_name in helper.get_input_stanza_names():\n        data = str(random.randint(0,100))\n        event = helper.new_event(source=input_type, index=helper.get_output_index(stanza_name), sourcetype=helper.get_sourcetype(stanza_name), data=data)\n        ew.write_event(event)\n    '''\n    ds_url = set_url(helper)\n    ds_api_key = helper.get_global_setting('dataset_log_read_access_key')\n    ds_headers = { \"Authorization\": \"Bearer \" + ds_api_key }\n    ds_start_time = helper.get_arg('start_time')\n    ds_end_time = helper.get_arg('end_time')\n    ds_query = helper.get_arg('dataset_query_string')\n    #ds_query = ds_query.replace(\"'\", \"\\\\'\")\n    ds_max_count = helper.get_arg('max_count')\n\n    ds_payload = { \"queryType\": \"log\", \"startTime\": ds_start_time }\n    if ds_end_time:\n        ds_payload['endTime'] = ds_end_time\n    if ds_query:\n        ds_payload['filter'] = ds_query\n    if ds_max_count:\n        ds_payload['maxCount'] = ds_max_count\n   \n    get_proxy = set_proxy(helper, ew)\n    \n    #write_events(helper, ew, str(json.dumps(ds_payload)))\n    try:\n        #handle proxy settings\n        if get_proxy is None:\n            r = requests.post(url=ds_url, headers=ds_headers, json=ds_payload)\n        else:\n            r = requests.post(url=ds_url, headers=ds_headers, json=ds_payload, proxies=get_proxy)\n\n        r_json = r.json() #parse results json\n        \n        #log information from results\n        if r_json['status']:\n            helper.log_info(\"response status=%s\" % str(r_json['status']))\n            \n        #response includes good information for debug logging\n        if r_json['status'] == 'success':\n            if r_json['executionTime']:\n                helper.log_debug(\"executionTime %s\" % (str(r_json['executionTime'])))\n            if r_json['cpuUsage']:\n                helper.log_debug(\"cpuUsage is %s\" % (str(r_json['cpuUsage'])))\n        \n        #if successful and results, parse the response\n        if r_json['status'] == 'success' and len(r_json['matches']) > 0:\n            \n            #parse results, match returned matches with corresponding sessions\n            matches = r_json['matches']\n            sessions = r_json['sessions']\n            \n            for match_list in matches:\n                ds_event_dict = {}\n                ds_event_dict = match_list\n                session_key = match_list['session']\n\n                for session_entry, session_dict in sessions.items():\n                    if session_entry == session_key:\n                        for key in session_dict:\n                            ds_event_dict[key] = session_dict[key]\n\n                ordered_ds_event_dict = OrderedDict(ds_event_dict)\n                #move timestamp to beginning for efficient Splunk timestamp parsing\n                ordered_ds_event_dict.move_to_end('timestamp', last=False)\n        \n                #check event time against checkpoint\n                #PowerQuery results are returned by default in chronological order\n                checkpoint_key = helper.get_input_type() + \"_key\"\n                event_time = int(ordered_ds_event_dict['timestamp'])\n                validated_checkpoint = compare_checkpoint(helper, checkpoint_key, event_time)\n        \n                #if greater than current checkpoint, update checkpoint and write event\n                if validated_checkpoint:\n                    helper.log_info(\"saving checkpoint %s\" % (str(event_time)))\n                    helper.save_check_point(checkpoint_key, event_time)\n                    ds_event = json.dumps(ordered_ds_event_dict)\n                    write_events(helper, ew, str(ds_event))\n                \n    except Exception as e:\n        write_events(helper, ew, str(e))\n        helper.log_error(e)\n\n\ndef write_events(helper, ew, data):\n    event = helper.new_event(source=helper.get_input_type(), index=helper.get_output_index(), sourcetype=helper.get_sourcetype(), data=data)\n    ew.write_event(event)", "customized_options": [{"name": "start_time", "value": "1m"}, {"name": "end_time", "value": ""}, {"name": "dataset_query_string", "value": "error"}, {"name": "max_count", "value": ""}], "uuid": "03489774c95a48f78c769de74b6ac9b1", "sample_count": 0}, {"index": "default", "sourcetype": "dataset:alerts", "interval": "60", "use_external_validation": true, "streaming_mode_xml": true, "name": "dataset_alerts", "title": "DataSet Alerts", "description": "Retrieve alerts from DataSet via API", "type": "customized", "parameters": [{"name": "start_time", "label": "Start Time", "help_string": "Relative time to query back. Use short form relative time, e.g.: 24h or 30d. Reference https://app.scalyr.com/help/time-reference", "required": true, "format_type": "text", "default_value": "24h", "placeholder": "24h", "type": "text", "value": "24h"}], "data_inputs_options": [{"type": "customized_var", "name": "start_time", "title": "Start Time", "description": "Relative time to query back. Use short form relative time, e.g.: 24h or 30d. Reference https://app.scalyr.com/help/time-reference", "required_on_edit": false, "required_on_create": true, "format_type": "text", "default_value": "24h", "placeholder": "24h"}], "code": "# encoding = utf-8\n\nimport os\nimport sys\nimport time\nimport datetime\nimport json\nimport requests\n\n'''\n    IMPORTANT\n    Edit only the validate_input and collect_events functions.\n    Do not edit any other part in this file.\n    This file is generated only once when creating the modular input.\n'''\n'''\n# For advanced users, if you want to create single instance mod input, uncomment this method.\ndef use_single_instance_mode():\n    return True\n'''\n\ndef validate_input(helper, definition):\n    \"\"\"Implement your own validation logic to validate the input stanza configurations\"\"\"\n    # This example accesses the modular input variable\n    # start_time = definition.parameters.get('start_time', None)\n    pass\n\n\n#set DataSet API url based on environment\ndef set_url(helper):\n    if helper.get_global_setting('dataset_eu_environment') == True:\n        return 'https://app.eu.scalyr.com/api/powerQuery'\n    else:\n        return 'https://app.scalyr.com/api/powerQuery'\n\n\n#set DataSet query\ndef set_ds_query(ds_start_time):\n    ds_query = { \"query\": \"tag=\\'alertStateChange\\' status=2 | columns timestamp, app, silencedUntilMs, gracePeriod, lastRedAlertMs, reportedStatus, renotifyPeriod, lastTriggeredNotificationMinutes, description, severity, trigger, lastStatus, status\", \"startTime\": ds_start_time }\n    return ds_query\n\n\n#get checkpoint, set initial value to 0 if nonexistent\ndef get_checkpoint(helper, key):\n    state = helper.get_check_point(key)\n\n    #if state exists, return it\n    if (state not in [None,'']):\n        return state\n    #if nonexistent, set to 0 for 1st iteration\n    else:\n        helper.log_info(\"checkpoint is 0\")\n        return 0\n\n\n#compare timestamp to checkpoint value\ndef compare_checkpoint(helper, key, data):\n    checkpoint_time = get_checkpoint(helper, key)\n    event_time = data\n\n    if checkpoint_time == 0 or (event_time > checkpoint_time):\n        return True\n    else:\n        helper.log_info(\"skipping due to event_time=%s is less than checkpoint=%s\" % (str(event_time), str(checkpoint_time)))\n        return False\n\n\n#parse proxy configuraton for requests module\ndef set_proxy(helper, ew):\n    if helper.get_proxy():\n        helper.log_debug(\"proxy enabled=true\")\n        proxy = helper.get_proxy()\n        proxies = {}\n        if proxy['proxy_username'] and proxy['proxy_password']:\n            proxies['http'] = proxy['proxy_username'] + \":\" + proxy['proxy_password'] + \"@\" + proxy['proxy_url'] + \":\" + proxy['proxy_port']\n        elif proxy['proxy_username']:\n            proxies['http'] = proxy['proxy_username'] + \"@\" + proxy['proxy_url'] + \":\" + proxy['proxy_port']\n        else:\n            proxies['http'] = proxy['proxy_url'] + proxy['proxy_port']\n        if proxy['proxy_type'] != 'http':\n            proxies['http'] = proxy['proxy_type'] + \"://\" + proxies['http']\n        \n        proxies['https'] = proxies['http']\n        return proxies\n        \n    else:\n        helper.log_debug(\"proxy enabled=false\")\n        return None\n\n\ndef collect_events(helper, ew):\n    \"\"\"Implement your data collection logic here\n\n    # The following examples get the arguments of this input.\n    # Note, for single instance mod input, args will be returned as a dict.\n    # For multi instance mod input, args will be returned as a single value.\n    opt_start_time = helper.get_arg('start_time')\n    # In single instance mode, to get arguments of a particular input, use\n    opt_start_time = helper.get_arg('start_time', stanza_name)\n\n    # get input type\n    helper.get_input_type()\n\n    # The following examples get input stanzas.\n    # get all detailed input stanzas\n    helper.get_input_stanza()\n    # get specific input stanza with stanza name\n    helper.get_input_stanza(stanza_name)\n    # get all stanza names\n    helper.get_input_stanza_names()\n\n    # The following examples get options from setup page configuration.\n    # get the loglevel from the setup page\n    loglevel = helper.get_log_level()\n    # get proxy setting configuration\n    proxy_settings = helper.get_proxy()\n    # get account credentials as dictionary\n    account = helper.get_user_credential_by_username(\"username\")\n    account = helper.get_user_credential_by_id(\"account id\")\n    # get global variable configuration\n    global_dataset_log_read_access_key = helper.get_global_setting(\"dataset_log_read_access_key\")\n    global_dataset_eu_environment = helper.get_global_setting(\"dataset_eu_environment\")\n\n    # The following examples show usage of logging related helper functions.\n    # write to the log for this modular input using configured global log level or INFO as default\n    helper.log(\"log message\")\n    # write to the log using specified log level\n    helper.log_debug(\"log message\")\n    helper.log_info(\"log message\")\n    helper.log_warning(\"log message\")\n    helper.log_error(\"log message\")\n    helper.log_critical(\"log message\")\n    # set the log level for this modular input\n    # (log_level can be \"debug\", \"info\", \"warning\", \"error\" or \"critical\", case insensitive)\n    helper.set_log_level(log_level)\n\n    # The following examples send rest requests to some endpoint.\n    response = helper.send_http_request(url, method, parameters=None, payload=None,\n                                        headers=None, cookies=None, verify=True, cert=None,\n                                        timeout=None, use_proxy=True)\n    # get the response headers\n    r_headers = response.headers\n    # get the response body as text\n    r_text = response.text\n    # get response body as json. If the body text is not a json string, raise a ValueError\n    r_json = response.json()\n    # get response cookies\n    r_cookies = response.cookies\n    # get redirect history\n    historical_responses = response.history\n    # get response status code\n    r_status = response.status_code\n    # check the response status, if the status is not sucessful, raise requests.HTTPError\n    response.raise_for_status()\n\n    # The following examples show usage of check pointing related helper functions.\n    # save checkpoint\n    helper.save_check_point(key, state)\n    # delete checkpoint\n    helper.delete_check_point(key)\n    # get checkpoint\n    state = helper.get_check_point(key)\n\n    # To create a splunk event\n    helper.new_event(data, time=None, host=None, index=None, source=None, sourcetype=None, done=True, unbroken=True)\n    \"\"\"\n\n    '''\n    # The following example writes a random number as an event. (Multi Instance Mode)\n    # Use this code template by default.\n    import random\n    data = str(random.randint(0,100))\n    event = helper.new_event(source=helper.get_input_type(), index=helper.get_output_index(), sourcetype=helper.get_sourcetype(), data=data)\n    ew.write_event(event)\n    '''\n\n    '''\n    # The following example writes a random number as an event for each input config. (Single Instance Mode)\n    # For advanced users, if you want to create single instance mod input, please use this code template.\n    # Also, you need to uncomment use_single_instance_mode() above.\n    import random\n    input_type = helper.get_input_type()\n    for stanza_name in helper.get_input_stanza_names():\n        data = str(random.randint(0,100))\n        event = helper.new_event(source=input_type, index=helper.get_output_index(stanza_name), sourcetype=helper.get_sourcetype(stanza_name), data=data)\n        ew.write_event(event)\n    '''\n    \n    ds_url = set_url(helper)\n    ds_api_key = helper.get_global_setting('dataset_log_read_access_key')\n    ds_headers = { \"Authorization\": \"Bearer \" + ds_api_key }\n    ds_start_time = helper.get_arg('start_time')\n    #use DataSet PowerQuery to limit returned fields\n    ds_payload = set_ds_query(ds_start_time)\n    \n    '''\n    example of PowerQuery results:\n    {\n    \"columns\": [\n        {\n            \"name\": \"timestamp\"\n        },\n        {\n            \"name\": \"app\"\n        },\n        {\n            \"name\": \"silencedUntilMs\"\n        },\n        {\n            \"name\": \"gracePeriod\"\n        },\n        {\n            \"name\": \"lastRedAlertMs\"\n        },\n        {\n            \"name\": \"reportedStatus\"\n        },\n        {\n            \"name\": \"renotifyPeriod\"\n        },\n        {\n            \"name\": \"lastTriggeredNotificationMinutes\"\n        },\n        {\n            \"name\": \"description\"\n        },\n        {\n            \"name\": \"severity\"\n        },\n        {\n            \"name\": \"trigger\"\n        },\n        {\n            \"name\": \"lastStatus\"\n        },\n        {\n            \"name\": \"status\"\n        }\n    ],\n    \"warnings\": [\n        \"Result set limited to 1000 rows by default. To display more rows, add a command like \\\"| limit 10000\\\".\"\n    ],\n    \"values\": [\n        [\n            1658762001620795544,\n            \"appserver\",\n            0,\n            0,\n            1658760799604,\n            1,\n            60,\n            20,\n            \"Delays over 7.5s\",\n            3,\n            \"count:2 minutes(timeMs > 7500) > 2\",\n            1,\n            2\n        ],\n        [\n            1658767160258414975,\n            \"cloudWatchLogs\",\n            0,\n            0,\n            1658766860380,\n            1,\n            0,\n            4,\n            \"Errors on cloudWatchLogs\",\n            3,\n            \"count:2 minutes((\\\"error\\\") ($serverHost contains \\\"cloudWatchLogs\\\"))/count:2 minutes(!(\\\"error\\\") ($serverHost contains \\\"cloudWatchLogs\\\")) > 0\",\n            1,\n            2\n        ],\n        ...<more value lists>...\n        \"matchingEvents\": 9.0,\n        \"status\": \"success\",\n        \"omittedEvents\": 0.0\n    '''\n    \n    get_proxy = set_proxy(helper, ew)\n    \n    try:\n        #handle proxy settings\n        if get_proxy is None:\n            r = requests.post(url=ds_url, headers=ds_headers, json=ds_payload)\n        else:\n            r = requests.post(url=ds_url, headers=ds_headers, json=ds_payload, proxies=get_proxy)\n\n        r_json = r.json() #parse results json\n        \n        #log information from results\n        if r_json['status']:\n            helper.log_info(\"response status=%s\" % str(r_json['status']))\n         \n        if r_json['warnings']:\n            for warning in r_json['warnings']:\n                helper.log_info(\"response warning=%s\" % str(warning))\n            \n        if r_json['matchingEvents']:\n            helper.log_info(\"response matches=%s\" % str(r_json['matchingEvents']))\n            \n        if r_json['omittedEvents']:\n            helper.log_warning(\"response omitted=%s\" % str(r_json['omittedEvents']))\n        \n        #parse results, match returned columns with corresponding values\n        ds_event_dict = {}\n        for value_list in r_json['values']:\n            for counter in range(len(value_list)):\n                ds_event_dict[r_json['columns'][counter]['name']] = value_list[counter]\n        \n            #check event time against checkpoint\n            #PowerQuery results are returned by default in chronological order\n            checkpoint_key = helper.get_input_type() + \"_key\"\n            event_time = ds_event_dict['timestamp']\n            validated_checkpoint = compare_checkpoint(helper, checkpoint_key, event_time)\n        \n            #if greater than current checkpoint, update checkpoint and write event\n            if validated_checkpoint:\n                helper.log_info(\"saving checkpoint %s\" % (str(event_time)))\n                helper.save_check_point(checkpoint_key, event_time)\n                ds_event = json.dumps(ds_event_dict)\n                write_events(helper, ew, str(ds_event))\n                \n    except Exception as e:\n        helper.log_error(e)\n\n\ndef write_events(helper, ew, data):\n    helper.log_debug(\"writing event %s\" % data)\n    event = helper.new_event(source=helper.get_input_type(), index=helper.get_output_index(), sourcetype=helper.get_sourcetype(), data=data)\n    ew.write_event(event)\n", "customized_options": [{"name": "start_time", "value": "24h"}], "uuid": "27052499ae3243a29d2dcdc3fe0b48f2", "sample_count": 0}]}, "global_settings_builder": {"global_settings": {"log_settings": {"log_level": "DEBUG"}, "customized_settings": [{"required": false, "name": "dataset_eu_environment", "label": "DataSet EU Environment", "default_value": false, "help_string": "Select if DataSet environment is https://app.eu.scalyr.com, leave unselected for default https://app.scalyr.com", "type": "checkbox", "format_type": "checkbox", "value": 0}, {"required": false, "name": "dataset_log_read_access_key", "label": "DataSet Log Read Access Key", "placeholder": "", "default_value": "", "help_string": "Required to enable inputs and SPL comand. Include trailing hyphens if applicable.", "type": "password", "format_type": "password", "value": ""}, {"required": false, "name": "dataset_log_write_access_key", "label": "DataSet Log Write Access Key", "placeholder": "", "default_value": "", "help_string": "Required to enable alert action. Include trailing hyphens if applicable.", "type": "password", "format_type": "password", "value": ""}]}}, "sourcetype_builder": {"dataset:alerts": {"metadata": {"event_count": 0, "data_input_name": "dataset_alerts", "extractions_count": 0, "cims_count": 0}}, "dataset:query": {"metadata": {"event_count": 0, "data_input_name": "dataset_query", "extractions_count": 0, "cims_count": 0}}}, "validation": {"validators": ["best_practice_validation", "data_model_mapping_validation", "field_extract_validation", "app_cert_validation"], "status": "job_started", "validation_id": "v_1659625695_76"}}